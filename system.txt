//File Tree:
backend/
│
├── scripts/                 # Scripts directory (inferred from manifest.json)
│   ├── content-script.js   # Content script for browser extension (referenced in content_scripts of manifest.json)
│   └── service-worker.js   # Background service worker for browser extension
|   |-- embeddings.js.      # Embedding logic for sense similarity to search DOM document and return most related sense to query
│
├── scripts/                  # Assuming a common scripts directory for server-side JavaScript
│   ├── index.js             # Main server file (entry point for Express server)
│   └── sendToLocalLLM.js   # Functionality to handle user input and communicate with LLM server
│
├── sidepanel.html            # Side panel HTML file (referenced in default_path of manifest.json)
│
├── icons/                   # Icons directory (inferred from default_icon in manifest.json)
│   └── rambot_logo.png     # Logo icon for the browser extension
│
├── package.json              # Project metadata and dependencies (common file for Node.js projects)

//service-worker.js:
// Listen for the extension's installation to set up any initial configuration
chrome.runtime.onInstalled.addListener(() => {
  chrome.contextMenus.create({
      id: 'openSidePanel',
      title: 'Open side panel',
      contexts: ['all']
  });
  console.log("Sidepanel opened");
});

// Add a listener for incoming messages in service-worker.js or another background script
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  if (message.action === 'sendToBackend') {
    // Handle the OpenAI API request here using message.inputText as the input text data
    const apiKey = 'lm-studio';
    const apiUrl = 'https://localhost:1234/v1/chat/completions';
    
    sendToOpenAI(message.inputText, apiKey, apiUrl); // Call the API request function
  }
});

async function fetchData(query) {
  // Replace this with your actual data fetching logic
  // For example, you might make an AJAX request to a server-side API
  return new Promise(resolve => {
      setTimeout(() => {
          resolve([
              { content: `Result for query "${query}"` }
          ]);
      }, 1000);
  });
  
}

chrome.tabs.onUpdated.addListener((tabId, changeInfo, tab) => {
  if (changeInfo.status === 'complete') {
    fetchPageContent(tab.url);
  } 
});

async function fetchPageContent(url) {
  const response = await chrome.tabs.sendMessage(tabId, { action: "captureDom" });
  const html = response.domContent;
  // Pass the HTML content to your Retrieval Augmented Generative model
  // ...
}


chrome.tabs.executeScript(null, {
  code: `document.documentElement.innerHTML`,
}, (results) => {
  // Send the DOM content back to the extension
});

---

//sendToLocalLLM.js:
// Send a ready signal to the service worker
chrome.runtime.sendMessage({ action: "sendToOpenAi-Ready" });
class LlamaIndex {
  constructor(index) {
      this.index = new InvertedIndex(index);
  }

  async searchDocuments(query, k = 5) {
      return this.index.search(query, k);
  }
}

// Initialize the llama_index instance
const docStorage = new EmbeddingDocumentStorage(); // This will store documents as embeddings
const index = new LlamaIndex(new SimpleVectorScoreScorer());
docStorage.addToIndex(index);




let currentDomContent = "No DOM content available."; // domContent is a global variable that gets updated with the current DOM content.
let domUpdated = false; // Used to control token count and prevent over billing

// Function to update DOM content globally
function updateDomContent(newDomContent) {
  currentDomContent = newDomContent;
  domUpdated = true;
}

async function startConversationWithLocalLLM() { 
  const apiKey = 'not-needed'; 
  const apiUrl = 'http://localhost:1234/v1';

  let message_content = [
    { "role": "system", "content": "You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful." },
    { "role": "user", "content": "Hello, introduce yourself to someone opening this program for the first time. Be concise." },
    { "role": "assistant", "content": "Hello! I am a codegemma-2b model trained on code. I am here to help you with your questions." },
  ];

  if (combinedContext !== "") {
    // Add combined context if available
    message_content.unshift({ "role": "system", "content": combinedContext });
  }

  // Data object containing the input text
  const data = {
    messages: message_content, 
    model: 'llama3:8b',
    max_tokens: 1024, // Adjust this as needed, it represents the maximum number of tokens (words) the model generates
    temperature: 0.4 // Adjust this as needed, it controls the randomness of the generated text
  };


  try {
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify(data),
      credentials: 'omit' // Ensure no cookies are sent
    });

    if (!response.ok) throw new Error('Network response was not ok');

    console.log('Conversation thread with Local LLM created');
    const responseData = await response.json();
    const content = responseData.choices[0].message.content; // Access the content from the response
    console.log('Greeting from Local LLM ', content);
    
    // Update conversation history with current interaction
    updateConversationHistory(inputText, content);


    // Display response in responseContainer
    const responseContainer = document.getElementById('responseContainer');
    responseContainer.innerHTML = `<p>${content}</p>`; // Displaying the content
  } catch (error) {
    console.error('Failed to create conversation thread with Local LLM:', error);
    const responseContainer = document.getElementById('responseContainer');
    responseContainer.innerHTML = `<p>Failed to create conversation thread with ChatGBT. Please try again later.</p>`; // Displaying error message to user
    console.error('Error type:', error.name); // Print the error type to console
  }
}

// Listens for message from service worker 
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  if (message.action === "sendDomToGBT") { // Message from service worker that contains the current DOM
    updateDomContent(message.domContent);
    console.log('sendToOpenAi.js - SUCCESS: Recieved DOM from service-worker.js. Now updating current DOM...');
    // console.log(message.domContent);
  }
  else if (message.action === "startGBTConvo") { // Message from service worker that says to start conversation with chatGBT
    startConversationWithLocalLLM(); 
  }
});


// Event listener for button click
document.getElementById('sendDataButton').addEventListener('click', function () {
  const textInput = document.getElementById('textInput');
  const inputData = textInput.value;

  // Asynchronously send the message
  sendtoLocalLLM(inputData).then(() => {
    textInput.value = ''; // Clear the input field after the message is sent
  }).catch(error => {
    console.error('Failed to send message:', error);
    textInput.value = ''; // Ensure input field is cleared even if sending fails
  });

  console.log('Sending message to OpenAi ChatGBT');
});

// Chat widget embed code
const chatWidgetScript = document.createElement('script');
chatWidgetScript.setAttribute('data-embed-id', '5fc05aaf-2f2c-4c84-87a3-367a4692c1ee');
chatWidgetScript.setAttribute('data-base-api-url', 'https://openai.com//*');
document.head.appendChild(chatWidgetScript);


// Function to update conversation history with each interaction
function updateConversationHistory(input, output) {
  conversationHistory.push({ input, output });
}

// Function to build conversation context from history
function buildConversationContext() {
  let context = "";
  for (const interaction of conversationHistory) {
    context += `${interaction.input}\n${interaction.output}\n`;
  }
  return context;
}


  async function sendtoLocalLLM(inputData, model = "codegemma/codegemma-2b", useGBT = false) {
    const apiKey = 'not-needed'; // Only used when useGBT is true
    const localUrl = 'http://localhost:5000/v1/chat/completions';
    let messages = inputData; // Assume inputData can be either a string or an array of message objects
    let apiEndpoint = localUrl;
    const headers = {
      'Content-Type': 'application/json',
      ...(useGBT && {'Authorization': `Bearer ${apiKey}`}) // Add auth header only when using GBT
  };

    // Build conversation context from history
    const conversationContext = buildConversationContext();

  if (useGBT) {
    messages = [
      {"role": "system", "content": "You are a virtual assistant who uses the DOM of the webpages a user visits as context."},
      {"role": "user", "content": messages} // Wrap the inputData in a user message object if it's a string
    ];
  } else {
    messages = messages.replace(/\\n/g, " "); // Replace newlines with spaces for local API
  }

  const data = useGBT ? {messages: messages, model: 'llama3:8b', max_tokens: 1024, temperature: 0.4} : {model: model, prompt: messages, max_tokens: max_tokens, temperature: temperature};

  try {
    const response = await fetch(apiEndpoint, {
      method: 'POST',
      headers: headers,
      body: JSON.stringify(data),
      credentials: useGBT ? 'omit' : undefined // Ensure no cookies are sent when using GBT
    });

    if (!response.ok) throw new Error('Network response was not ok');

    const responseData = await response.json();
    let content;
    if (useGBT) {
      content = responseData.choices[0].message.content; // Parse GBT's nested response format
    } else {
      content = responseData.response; // Parse local API's flat response format
    }

    updateConversationHistory(inputData, content);
    displayResponse(content); // Abstracted DOM manipulation into a separate function
  } catch (error) {
    console.error(`Error sending data to ${apiEndpoint}:`, error);
    displayError('Failed to create conversation thread. Please try again later.');
    console.error('Error type:', error.name);
  }
}

function displayResponse(content) {
  const responseContainer = document.getElementById('responseContainer');
  responseContainer.innerHTML = `<p>${content}</p>`;
}

function displayError(message) {
  const responseContainer = document.getElementById('responseContainer');
  responseContainer.innerHTML = `<p>${message}</p>`;
}

async function getEmbedding(text, model = "nomic-ai/nomic-embed-text-v1.5-GGUF") {
  text = text.replace(/\\n/g, " "); // Replace newlines with spaces
  const response = await fetch(`http://localhost:5000/embdings`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ input: [text], model: model })
  });

  if (!response.ok) throw new Error('Network response was not ok');
  const data = await response.json();
  return data.embeddings[0].embedding;
}

// Listeners for messages from service worker
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  if (message.action === "sendDomToGBT") { // Message from service-worker.js that contains the current DOM
    updateDomContent(message.domContent);
    console.log('sendtoLocalLLM.js - SUCCESS: Recieved DOM from service-worker.js. Now updating current DOM...');
  }

  else if (message.action === "startGBTConvo") { // Message from service-worker.js that says to start conversation with chatGBT
    sendtoLocalLLM("Hello, I'm ready to chat!").then(() => {
      // Additional logic after starting the conversation
    });
  }
});

// Event listener for button click
document.getElementById('sendDataButton').addEventListener('click', function () {
  const textInput = document.getElementById('textInput');
  const inputData = textInput.value;

  // Asynchronously send the message
  sendToOpenAI(inputData).then(() => {
    textInput.value = ''; // Clear the input field after the message is sent
  }).catch(error => {
    console.error('Failed to send message:', error);
    textInput.value = ''; // Ensure input field is cleared even if sending fails
  });

  console.log('Sending message to OpenAi ChatGBT');
});




// Function
function handleUserInput(userMessage) {
    const newUserMessage = { "role": "user", "content": userMessage };
    addContextToHistory(newUserMessage).then(() => {
        return getContextForMessage(newUserMessage);
    }).then(contextMessage => {
        if (contextMessage) {
            message_content.push(contextMessage);
        }
        return sendChatCompletionToLLM(message_content);
    }).then(assistantMessage => {
        message_content.push({ "role": "assistant", "content": assistantMessage });
    }).catch(error => {
        console.error('An error occurred:', error);
    });
}

// Event listener or other pattern to handle user input asynchronous events
function handleUserInput(userMessage) {
  const newUserMessage = { "role": "user", "content": userMessage };
  
  getContextForMessage(newUserMessage).then(contextMessage => {
      if (contextMessage) {
          history.push(contextMessage);
      }
      
      // Use fetch to create chat completions
      fetch(`${baseUrl}/chat/completions`, {
          method: 'POST',
          headers: {
              'Content-Type': 'application/json',
              // Add any necessary headers for authentication
          },
          body: JSON.stringify({
              model: "bartowski/aixcoder-7b-GGUF",
              messages: message_content,
              temperature: 0.7
          })
      }).then(response => response.body.pipe(new Response()))
      .then(response => response.text())
      .then(text => {
          // Process the streamed text and update the assistant's message in history
          const newAssistantMessage = { "role": "assistant", "content": text };
          message_content.push(newUserMessage);
          message_content.push(newAssistantMessage);
      });
  });
}

// New function to add context to the message_content
function addContextToHistory(message) {
    return getContextForMessage(message).then(contextMessage => {
        if (contextMessage) {
            message_content.push(contextMessage);
        }
    });
}

// New function to send chat completion to OpenAI
function sendChatCompletionToLLM(message_content) {
    return fetch(`${baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            // Add any necessary headers for authentication
        },
        body: JSON.stringify({
            model: "bartowski/aixcoder-7b-GGUF",
            messages: message_content,
            temperature: 0.7
        })
    }).then(response => response.text()).then(text => text);
}


// Example usage for a Node.js environment or similar
const readline = require('readline').createInterface({
    input: process.stdin,
    output: process.stdout
});

readline.prompt();

readline.on('line', (input) => {
    handleUserInput(input);
    readline.prompt();
}).on('close', () => {
    console.log('Have a great day!');
    process.exit(0);
});

---

//sendToBackend.js:
// sendToBackendScript.js
console.log('This is a popup!');
const { runtime } = chrome;

async function sendtoLocalLLMWrapper(inputText) {
  try {
    // Call sendtoLocalLLM function, passing inputText as message content
    await sendtoLocalLLM(inputText);
  } catch (error) {
    console.error('Error sending data to Local LLM:', error);
    // Handle error here, e.g., display it to the user
  }
}

// New function name: sendToBackground
const sendToBackground = async (inputText) => {
  const apiUrl = 'http://localhost:3001/api/completions';
  const apiKey = 'not-needed'; // Replace with your actual API key
  try {
    // Define the message we want to send to the background script
    const message = { action: "sendToBackend", inputText };
    
    // Send a fetch request instead of using chrome.runtime.sendMessage
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        prompt: inputText,
        max_tokens: 150 // Adjust as needed
      })
    });

    const data = await response.json();
    console.log('Response from backend API:', data);
    // Handle response here, e.g., display it to the user
  } catch (error) {
    console.error('Error sending message to background script:', error);
    // Handle error
  }
};

// New event listener for sendDataButton click
document.getElementById('sendDataButton').addEventListener('click', async () => {
  const inputData = document.getElementById('textInput').value;
  await sendToBackground(inputData); // Call the new function with the input text value
});

// Chat widget embed code
const chatWidgetScript = document.createElement('script');
chatWidgetScript.setAttribute('data-embed-id', '5fc05aaf-2f2c-4c84-87a3-367a4692c1ee');
chatWidgetScript.setAttribute('data-base-api-url', 'http://localhost:3001/*');
chatWidgetScript.setAttribute('src', 'http://localhost:3000/embed/anythingllm-chat-widget.min.js');
document.head.appendChild(chatWidgetScript);

---

//content-script.js:
function createDataHandler(requestType) {
  return function(url, data = {}) {
    const defaultOptions = {
      method: requestType,
      headers: { 'Content-Type': 'application/json' },
      ...(data && { body: JSON.stringify(data) })
    };
    return performRequest(url, defaultOptions);
  };
}

const fetch = createDataHandler('GET');
const post = createDataHandler('POST');

async function performRequest(url, options) {
  try {
    const response = await fetch(url, options);
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    return response.json(); // debugging return
  } catch (error) {
    console.error(`An error occurred while performing a request:`);
    throw error;
  }
}

function buildRequestOptions(method, url, body = {}) {
  return Object.assign({ method, headers: { 'Content-Type': 'application/json' } }, body);
}


// Function to update the DOM content of a specific element on the page
function updateDomContent(domElementId, newContent) {
  const domElement = document.getElementById(domElementId);
  if (domElement) {
    domElement.innerHTML = newContent;
    return true;
  } else {
    throw new Error(`Element with ID ${domElementId} not found in the DOM.`);
  }
}



async function withExponentialBackoff(task, maxAttempts = 3, initialDelay = 100) {
  let delay = initialDelay;
  for (let attempts = 0; attempts < maxAttempts; attempts++) {
    try {
      return await task();
    } catch (error) {
      console.error(`Attempt ${attempts + 1} failed:`, error);
      await new Promise(resolve => setTimeout(resolve, delay));
      delay *= 2;
    }
  }
  throw new Error('Maximum attempts reached. Task failed.');
}

async function captureAndSendDOM() {
  await withExponentialBackoff(async () => {
    const domContent = await captureDomContent();
    chrome.runtime.sendMessage({ action: 'update-dom', content: domContent });
  });
}


// Function to get the current URL of the active tab in the browser
async function getCurrentTabUrl(tabId) {
  let queryOptions = { active: true, currentWindow: true };
  // `tab` will either be a `tabs.Tab` instance or `undefined`.
  let [tab] = await chrome.tabs.query(queryOptions);
  if (tab && tabId === tab.id) {
    return tab.url;
  } else {
    throw new Error('Could not find the current active tab.');
  }
}

// Function to capture DOM content from the page and send it to the background script
async function processDOMIfNewPage() {
  const url = await getCurrentTabUrl(tabId);
  if (url !== cachedUrl) {
    // Update the cached URL
    cachedUrl = url;
    try {
      await captureAndSendDOM();
    } catch (error) {
      console.error('Failed to process DOM for new page:', error);
    }
  } else {
    console.log('The current tab has not changed, no need to update the DOM content.');
  }
}

// Function to add context to the history without duplicates
async function addContextToHistory(message) {
  const contextMessage = await getContextForMessage(message);
  if (contextMessage && !history.has(contextMessage)) {
    history.push(contextMessage);
  }
}

// Function to send chat completion request to language generation model
async function sendChatCompletionToLLM(messages) {
  const response = await fetch(`${baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      // Add any necessary headers for authentication
    },
    body: JSON.stringify({
      model: "bartowski/aixcoder-7b-GGUF",
      messages,
      temperature: 0.7
    })
  });
  return await response.text();
}

// Function to handle user input and start a conversation with the bot
async function handleUserInput(userMessage) {
  const newUserMessage = { "role": "user", "content": userMessage };

  try {
    await addContextToHistory(newUserMessage);
    const contextMessage = await getContextForMessage(newUserMessage);
    if (contextMessage) {
      history.push(contextMessage);
    }

    const assistantMessage = await sendChatCompletionToLLM([newUserMessage, ...history]);
    const newAssistantMessage = { "role": "assistant", "content": assistantMessage };

    console.log(`Assistant says: ${newAssistantMessage.content}`); // Log the assistant's response to the console
    history.push(newAssistantMessage);
  } catch (error) {
    console.error('An error occurred:', error);
  }
}

// Function to handle chat input via command line argument
const args = program.parse(process.argv);
async function run(args) {
  await handleUserInput(args._[0]); // Assuming user message is the first argument
  readlineInterface.close();
}

// Expose history and addContextToHistory for testing or further use if necessary
module.exports = {
  history: Array.from(history), // Use Array.from to create a shallow copy of the Set
  addContextToHistory,
  run
};


processDOMIfNewPage(); // Run at document end to ensure the DOM is fully loaded

---

// embeddings.js
module.exports = EmbeddingModel;


const EmbeddingModel = require('./scripts/embedding_model');
const axios = require('axios');
const SentenceTransformer = require('sentence-transformers');


class EmbeddingModel {
  constructor() {
    this.model = new SentenceTransformer('nomic-ai/nomic-embed-text-v1.5-GGUF').then(model => model);
   }

  async generateEmbedding(text) {
    const model = await this.model; // Ensure the model is loaded before generating embeddings]
    this.getEmbedding
    return model.encode(text);
  }

  async searchDocumentsSimilarity(query, topK) {
    const queryEmbedding = await this.generateEmbedding(query);
    let similarities = {};

    for (const [document, embedding] of Object.entries(this.documentEmbeddings)) {
      const cosineSimilarity = this.cosineSimilarity(queryEmbedding, embedding);
      similarities[document] = cosineSimilarity;
     }

     // Sort documents by similarity score in descending order and return the top K
    return Object.entries(similarities)
       .sort((a, b) => b[1] - a[1])
       .slice(0, topK)
       .map(entry => entry[0]); // Return only document names
  }

  async cosineSimilarity(vecA, vecB) {
    const dotProduct = vecA.reduce((sum, a, idx) => sum + a * vecB[idx], 0);
    const magnitudeA = Math.sqrt(vecA.reduce((sum, val) => sum + val ** 2, 0));
    const magnitudeB = Math.sqrt(vecB.reduce((sum, val) => sum + val ** 2, 0));
    return dotProduct / (magnitudeA * magnitudeB);
  }
  async generateEmbedding(text) {
      if (this.embeddingCache[text]) {
        return this.embeddingCache[text];
      }
      const embedding = await this.embeddingModel.generateEmbedding(text);
      this.embeddingCache[text] = embedding;
      return embedding;
    }

  async sendToBackend(endpoint, data) {
    try {
      const response = await axios.post(`http://localhost:5000/${endpoint}`, data);
      return response.data;
     } catch (error) {
       // Handle errors appropriately
      throw new Error(`Error sending data to backend: ${error}`);
  }
  }

  async sendToOpenAI(prompt) {
    const data = {
      model: 'codegemma/codegemma-2b',
      prompt: prompt,
      max_tokens: 150,
      temperature: 0.5,
     };
    try {
      const response = await axios.post('https://api.openai.com/v1/engines/codegemma/completions', data, {
        headers: {
           'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
           'Content-Type': 'application/json',
         },
       });
      return response.data;
     } catch (error) {
       // Handle errors appropriately
      throw new Error(`Error sending data to OpenAI: ${error}`);
    }
  }

  async getEmbedding(inputText) {
    try {
      const response = await fetch('http://localhost:3000/embeddings', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ input: [inputText] })
      });
  
      const { embeddings } = await response.json();
      return embeddings[0];  // Assuming only one embedding is needed
    } catch (error) {
      console.error('Error retrieving embeddings:', error);
    }
  }

  // Function to update DOM content globally
  async updateDomContent(newDomContent) {
  currentDomContent = newDomContent;
  domUpdated = true;
  }
}
